\documentclass[10pt,a4paper]{article}
\usepackage{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[mag=1000,a4paper,left=2.2cm,right=3.0cm,top=2.1cm,bottom=3.0cm]{geometry}
\usepackage{graphicx}
\usepackage{amsfonts,amssymb,amscd,amsmath,amsthm}
\usepackage{mathrsfs}
\usepackage{epsf}
\usepackage{wrapfig}
\usepackage{cite}
\usepackage[hyper]{amsbib}
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage[colorlinks]{hyperref}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[]{algorithm2e}
\usepackage{listings}

\lstset{
  breaklines=true,
  numbers=left,
  showstringspaces=false
}

 % Цвета для гиперссылок
\definecolor{linkcolor}{HTML}{799B03} % цвет ссылок
\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок
 
\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}


\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{НИС Введение в специальность}
\fancyhead[R]{\thepage}
\fancyfoot[CO]{\thepage}

\title{\textbf{Классификация задач в предметной области на основе статей из Web of Science}}
\author{\large{Мирошник В.} \\
\footnotesize{miroshnik.valerij.98@gmail.com} \\
\small{Научный руководитель: Гуськова М.}\\
\footnotesize{maria.guskova@rambler.ru}
}
\date{}
\def\W{\overset{\circ}{W}}
\newcommand{\ve}{\varepsilon}
\newcommand{\eps}{\varepsilon}

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newtheorem{theor}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{dfn}{Определение}

\begin{document}
\begin{center}
\large{Национальный исследовательский университет "Высшая школа экономики"\\
Московский институт электроники и математики\\
Департамент прикладной математики\\}
\end{center}



\begin{minipage}{\textwidth}
   \maketitle
\end{minipage}

\begin{abstract}
	\textbf{В данной работе реализована программа, которая получает информацию об интересующих пользователя статьях с сайта Web of Science.
	На основе этой информации строится граф цитирования, затем производится кластеризация статей методами K-Means и latent Dirichlet 
	allocation (LDA), и их анализ с помощью multidimensional scaling (MDS) и hierarchical document clustering (HDC).}\\
	
\end{abstract}

\normalsize

\section*{Введение}

%В введении необходимо кратко в общих словах ввести читателя в содержание работы. Требуется указать поставку задачи, методы, используемые в ходе работы, изложить основные идеи статьи и рассказать о полученных результатах. 
%В конце введения следует обозначить структуру статьи. В двух словах пояснить содержание каждого раздела: в первой главе рассматривается история поднятой проблемы4; во второй обосновывается выбор методов и инструментов, использованных в ходе работы; подробные описание проделанной работы описано в главе 3, а в четвертой обсуждаются полученные в ходе опытов результаты; в заключении содержатся выводы. 
Web of Science (WoS) — поисковая платформа, объединяющая базы данных публикаций в научных журналах и патентов.
WoS охватывает материалы по естественным, техническим, общественным, гуманитарным наукам и искусству. 
Платформа обладает встроенными возможностями поиска, анализа и управления библиографической информацией.\\
Задача, решаемая в рамках данной работы, заключается в том, чтобы научиться по запросу пользователя скачивать необходимые 
данные с WoS (названия статей, имена авторов, аннотации, цитируемые статьи), строить на основе этих данных граф цитирования, 
классифицировать статьи, решаемые в заданной пользователем области. Для классификации используются метод k-средних в 
совокупности с методом "локтя" (elbow method), латентное размещение Дирихле (LDA). Для анализа данных применены метод многомерного 
масштабирования (MDS) и иерархическая кластеризация.\\
Саму программу можно найти \href{https://github.com/Gorogorov/keks_and_lols/tree/master/Search_in_WoS}{в данном репозитории на GitHub}. 
Также по данной ссылке располагаются инструкции по запуску на платформах Windows и Linux.\\\\
\textbf{Структура статьи:}\\
\textbf{Глава I: Методы решения поставленной задачи} \\
Кратко обозреваются библиотеки, использованные для сбора данных с WoS и построения графа цитирования, 
рассматриваются методы анализа и кластеризации текста\\
\textbf{Глава II: Описание программы}\\
Подробное описание проделанной работы. Для наглядности данная глава поделена на 3 части:\\
1) Сбор данных и приведение их к виду, пригодному для анализа\\
2) Проведение идентификации статей и построение графа цитирования\\
3) Классификация статей методами k-means и LDA\\
\textbf{Глава III: Анализ результатов}\\
Анализ полученных результатов с помощью MDS и иерархической кластеризации, их обсуждение.\\
\textbf{Глава IV: Заключение}


\section{Методы решения поставленной задачи}
%Обзор литературы

%У любой задачи есть предыстория. Кто до вас занимался этим вопросом, какие результаты были получены, почему задача является актуальной. При написании обзора литературы, не забывайте про ссылки на источники. В учебно-методическом пособии \cite{latex} разобраны основные команды для работы с системой LaTex

Для того, чтобы решить данную задачу, во-первых, нужно научиться собирать данные с WoS. Основная проблема состоит в том,
что все ссылки зашифрованы, и совершенно не ясно, по какому принципу они формируются. Ее решение заключается в использовании 
Web Scraping. Суть метода в том, чтобы не просто качать страницы по ссылкам, а действительно открывать браузер, указывать программе
нужные html-формы, кликать по нужным кнопкам, по пути собирая данные для анализа. Для того, чтобы это реализовать, в данной 
программе используется библиотека selenium- инструмент для автоматизации действий веб-браузера.\\
\\
Второе - построение графа. Для данной задачи в программе при помощи NetworkX строится граф на основе уже полученных
данных, затем он импортируется в формат gexf. Графы, хранящиеся в данном формате, можно легко визуализировать с помощью 
программы Gephi. При наведении на вершину (в данном случае вершина равно статья) показываются все статьи, которые 
на нее ссылаются. Размер вершины зависит от количетсва входящих в нее ребер: чем их больше, тем больше и размер.

\subsection{Векторная модель}
Наконец, основная задача: кластеризация и анализ статей. Для того, чтобы каждую статью отнести к тому или иному кластеру, нужно
научиться сравнивать их аннотации. Делать это можно, построив векторную модель: представить коллекцию аннотаций векторами из одного 
общего для всей коллекции векторного пространства.\\
Документ в векторной модели представляется как множество термов. Термами называют слова, которые употреблены в тексте, а также такие
элементы текста, как, например, '2018' или 'Аполлон-13'. Важно научиться определять "вес" терма: то, насколько он "важен" для 
идентификации данной аннотации. Теперь, если представить каждую аннотацию как вектор $d_i$, а "веса" всех термов коллекции 
упорядочить и обозначить как $w_{ij}, j \in [n]$, где $n$ - количество термов в коллекции, то каждый из текстов можно записать в 
виде выражения
$$d_i = (w_{i1}, ..., w_{in})$$
Теперь, когда каждая аннотация представлена в виде вектора, можно решать задачу подобия: если обозначить каждый текст за точку, то
чем ближе точки, тем более похожи текста.\\
Каким же образом задавать веса для термов? Для этого существует несколько стандартных методов:\\
1) Булевский вес — равен 1, если терм встречается в документе и 0 в противном случае;\\
2) $TF$ (term frequency) — вес зависит только от частоты употребления терма в документе;\\
3) $TF\text{-}IDF$ (term frequency — inverse document frequency)\cite{tfidf} — вес зависит не только от частоты употребления слова в данном документе,
но и от частоты его употребления в остальных документах коллекции\\
Ясно, что первые 2 метода более грубые по сравнению с третьим, поэтому для решения задачи был выбран именно он.\\
TF зависит только от частоты употребления слова в данном документе и рассчитывается по формуле
$$TF(t, d) = \frac{n_t}{\sum_{k} n_k}$$
где $n_t$ - количество употреблений терма $t$ в документе $d$, в знаменателе общее количество термов в $d$\\
IDF - инверсия частоты, с которой слово встречается в документах коллекции. Чем более уникально слово, тем меньше у него IDF. И
наоборот, чем слово более употребимо, тем больше у него IDF. Вычисляется данная величина по формуле
$$IDF(t, D) = \log{\frac{|D|}{|\{d_i \in D | t \in d_i\}|}}$$
где $|D|$ - количество документов в коллекции $D$, $d_i$ - $i$-ый документ коллекции, $t$ - терм\\
Наконец, "вес" терма с помощью метода $TF\text{-}IDF$ вычисляется как
$$TF\text{-}IDF(t, d, D) = TF(t, d) \cdot IDF(t, D)$$
Теперь ясно, как представлять каждую из статей в виде вектора. Однако теперь нужно вернуться немного назад и понять, как для
каждой статьи задать набор корректных термов. В данной программе для этого используется модуль nltk (natural language toolkit). 
Считывая из базы данных аннотацию, из нее сразу же отбрасываются так называемые стоп-слова: предлоги, частицы и т.д.
Далее производится стемминг — процесс нахождения основы слова для заданного исходного слова. По полученным таким образом
термам строится матрица $TF\text{-}IDF$

\subsection{Кластеризация}
Кластеризация - задача упорядочивания множества объектов в сравнительно однородные группы \\
Существует множество методов кластеризации текстовых документов\cite{klast}. Для решения поставленной задачи были выбраны два из них:
метод k-средних и латентное размещение Дирихле. \\
\begin{center}
\textbf{Метод k-средних}
\end{center}
Данный метод разбивает множество документов на заранее известное число кластеров $k$. На каждой итерации алгоритма перерасчитываются 
центры кластеров так, чтобы минимизовать квадраты расстояний между центрами кластеров и элементами, которые соответствуют этим 
центрам.\cite{kmeans} Алгоритм завершается, как только на какой-то итерации перестает изменяться внутрикластерное расстояние. Это 
происходит за конечное число шагов, т.к. количество разбиений конечного множества конечно, а внутрикластерное расстояние на каждом 
шаге уменьшается (рис.1).\\
Более формально, нужно найти
$$\underset{S}{\mathrm{argmin}} = \sum_{i=1}^{k} \sum_{x \in S_i} \Vert x - \mu_i \Vert^2$$
Где $S$ - множество кластеров, $S_i$ - $i$-ый кластер, $\mu_i$ - центр $i$-ого кластера.\\\\
\fbox{
\begin{algorithm}[H]
 \KwData{$\{\vec{x_1}, ..., \vec{x_N}\}, K$ ($\vec{x_i}$ - $i$-ый документ, $K$ - количество кластеров, $N$ - количество документов)}
 \KwResult{$(\vec{\mu_1}, ..., \vec{\mu_K})$(оптимальные кластеры)}
 $(\vec{s_1}, ..., \vec{s_K}) \leftarrow SelectRandomSeeds(\{\vec{x_1}, ..., \vec{x_N}\}, K)$\;
 \For{$k \leftarrow 1 $ \KwTo $K$}{
  $\vec{\mu_k} \leftarrow \vec{s_k}$\;
 }
 \While{$CentroidsChange()$}{
  \For{$k \leftarrow 1 $ \KwTo $K$}{
   $\omega_k \leftarrow \{\}$ (инициализируем кластеры)\;
  }
  \For{$n \leftarrow 1 $ \KwTo $N$}{
   $j \leftarrow \underset{j'}{\mathrm{argmin}} \Vert \vec{\mu_{j'}} - \vec{x_n} \Vert$ (поиск номера кластера, расстояние до которого от $n$-ого элемента наименьшее)\;
   $\omega_j \leftarrow \omega_j \cup \{\vec{x_n}\}$ (присоединение элемента к оптимальному кластеру)\;
  }
  \For{$k \leftarrow 1 $ \KwTo $K$}{
   $\vec{\mu_k} \leftarrow \frac{1}{|\omega_k|} \sum_{\vec{x} \in \omega_k} \vec{x}$ (перераспределение кластеров) \;
  }
 }
 \caption{Method K-Means}
\end{algorithm}
}
\hfill \break
\newline
Для того, что определить оптимальное число кластеров $k$, в программе используется \textbf{"метод локтя" (elbow method)}. Суть метода
заключается в том, чтобы построить график зависимости количества кластеров от суммы квадратов расстояний от центров кластеров до
элементов, которые им соответствуют (рис. 2). Наиболее оптимальное количество кластеров - точка, в которой сумма квадратов начинает
падать более плавно.\\

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{./images/kmeans.png}
    \caption{K-Means}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{./images/ElbowMethod.png}
    \caption{Elbow method}
  \end{minipage}
\end{figure}

\begin{center}
\textbf{Латентное размещение Дирихле}
\end{center}
Latent Dirichlet allocation (LDA) - порождающая статистическая модель, позволяющая узнавать сходства между объектами и распределять
их по группам.\cite{lda}\cite{lda2}\\
Для того, чтобы не углубляться в математику, здесь приведено только краткое и интуитивное описание алгоритма:\\

\begin{enumerate} 
\item для каждой темы $t$
    \begin{enumerate}
    \item выбрать вектор $\varphi_t$ (распределение слов в теме) по распределению $\phi_t \sim Dir(\beta)$;
    \end{enumerate}
\item для каждого документа \textbf{w}:
    \begin{enumerate}
    \item выбрать $N \sim Poisson(\xi)$  
    \item выбрать вектор $\theta_d \sim Dir(\alpha)$ - вектор "степени выраженности" каждой темы в этом документе;
    \item для каждого из $N$ слов $w_n$:
        \begin{enumerate}
        \item выбрать тему $z_n$ по распределению $Multinomial(\theta)$;
        \item выбрать слово $w_n$ из $p(w_n|z_n, \beta)$ с вероятностями, заданными в $\beta$;
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{./images/lda.png}
  \caption{Графическое представление LDA. Прямоугольник \textbf{$M$} представляет собой документы, в то время как прямоугольник \textbf{$N$} - повторяющийся выбор тем и слов в документе}
\end{figure}

\subsection{Анализ результатов}
После построения кластеров нужно попробовать понять, насколько хорошо они отображают реальное положение дел. Для этого в рамках
данной задачи применены два метода, дающие графическую интерпретацию построенным данным.\\
\begin{center}
\textbf{MDS}
\end{center}
Высокоразмерные данные, т.е. данные, имеющие размерность более трех, может быть трудно интерпретировать. Один из подходов к упрощению
состоит в том, чтобы предположить, что данные, представляющие интерес, лежат во вложенном многообразии в пространстве более 
высокого порядка. Если это многообразие имеет малую размерность, его можно визуализировать в низкоразмерном (размерности 2 или 3) 
пространсте(рис. 4).\\
Одним из алгоритмов, реализующих данную идею, является \textbf{Multidimensional scaling (MDS)}.\\
Если $\delta_{ij}$ - расстояние между точками $i$ и $j$ в пространстве $\mathbb{R}^n$, то MDS пытается найти такую функцию $f: \mathbb{R}^n 
\rightarrow \mathbb{R}^d$ ($d$ - размерность пространства, представление в котором ищется), чтобы расстояния $\delta_{ij} = f(d_{ij})$
между точками $i$ и $j$ в пространстве $\mathbb{R}^d$ максимально коррелировали с $d_{ij}$.\cite{mds}\\
Итоговый результат MDS - матрица c координатами $n$ точек в пространстве $\mathbb{R}^d$ 
\begin{center}
\begin{pmatrix}
    x_{11}       & x_{12} & \dots & x_{1d} \\
    x_{21}       & x_{22} & \dots & x_{2d} \\
    \hdotsfor{4} \\
    x_{n1}       & x_{n2} & \dots & x_{nd}
\end{pmatrix}
\end{center}
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{./images/mds.png}
  \caption{Графическое представление MDS. Преобразование трехмерного пространства в двумерное}
\end{figure}

\begin{center}
\textbf{Иерархическая кластеризация (HDC)}
\end{center}
Второй метод для анализа данных, использованный в данной программе, заключается в построении дендрограммы(рис. 5). Для ее построения 
используется алгоритм Уорда (Ward's method), минимизирующий на каждой итерации дисперсию внутри кластеров.\\
Для реализации алгоритма на каждом шаге находят такую пару кластеров, чтобы увеличение дисперсии после их слияния оказалось
минимальным из возможных.\cite{hdc} На начальном этапе все кластеры являются одиночными, т.е. каждому кластеру соответствует ровно один
документ. Для применения данного метода начальное расстояние между кластерами должно быть пропорционально квадрату евклидова 
расстояния.\\
Поэтому в начальный момент расстояния между документами определяются как квадрат евклидова расстояния:
$$d_{ij} = d(\{X_i\}, \{X_j\}) = \Vert X_i - X_j \Vert ^2$$

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.9]{./images/hdc.png}
  \caption{HDC}
\end{figure}

\newpage

\section{Описание программы}
В данном разделе шаг за шагом описано, как реализована программа. В отчете код часто обрезан с целью большей наглядности.
Для того, чтобы увидеть программу целиком, нужно проследовать в репозиторий, ссылка на который указана выше.\\
\newline
Чтобы понять общую структуру программы, полезно посмотреть на main.py. 

\fbox{\lstinputlisting[language=Python, title=main.py]{art_main.py}}

\newline
\newline
\hfill \break
\hfill \break

Помимо main.py, программа состоит еще из трех файлов: wos\_parser.py, wos\_graph.py и \\ wos\_clasterization.py. Эти файлы отвечают
соответственно за парсинг WoS, построение графа цитирования и кластеризацию статей. Для удобства пользователя предусмотрен 
выбор параметров. Без параметров программа скачивает статьи, строит граф и проводит классификацию методом $k$-средних. Из 
названий параметров понятно, за что отвечает каждый из них. Возможно, следует пояснить про "nodownload" - передав данный 
параметр, пользователь сможет не скачивать статьи с WoS, а воспользоваться теми, которые уже скачаны. При этом пользователь 
обязан указать их количество. \\

\subsection{Web Scraping}
В main.py из файла wos\_parser.py используются 3 функции: site\_parser, article\_parser и\\ correct\_articles. Разберем содержание
каждой из них.\\

\fbox{\lstinputlisting[language=Python, title=wos\_parser.site\_parser() (1), firstline=1, lastline=10]{art_wos_parser.py}}

Данная часть кода открывает браузер Firefox, переходит на сайт WoS, вводит в поисковую строку запрос, который пользователь 
раннее сообщил программе. Иллюстрация того, как выглядит html-форма, которую нужно искать для построения запроса к методу 
driver.find\_element\_by\_xpath, показана на рис. 6.\\
После отправки запроса в поисковик программа переходит на первую статью и скачивает содержимое страницы в папку data. Все страницы,
скачанные пользователем, не удаляются после завершения программы. Это нужно для того, чтобы, в случае необходимости, была возможность
их повторного использования.\\ 

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.55]{./images/wossearch.png}
  \caption{Поиск нужной html-формы}
\end{figure}

Нахождение количества статей, это число записывается в cnt\_pages.

\fbox{\lstinputlisting[language=Python, title=wos\_parser.site\_parser() (2), firstline=12, lastline=23]{art_wos_parser.py}}
\hfill \break
\newline
\hfill \break
Для каждой статьи сохраняется html-страница. wos\_parser.site\_parser() возвращает количество 
статей, выданных WoS по запросу пользователя.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_parser.site\_parser() (3), firstline=24, lastline=36]{art_wos_parser.py}}
\newline
Для дальнейшего парсинга создан абстрактный класс Article. Он служит для структурирования данных. По A.name, A.author, A.abstract 
сохраняются, соответственно, название статьи, ее автор и аннотация. По A.CitedReference соханяется список статей, на которые
ссылается данная.\\
\newline
\fbox{\lstinputlisting[language=Python, firstline=1, lastline=2]{art_abstract_data.py}}
\newline 
\hfill \break
\newline
Функция wos\_parser.article\_parser() собирает из html-страницы статьи всю нужную информацию. Одним из параметром является articles:
это список статей, который заполняется по ходу выполнения функции. Для примера приведено получение из html-страницы статьи ее
аннотации.\\
Когда в WoS мы проводим поиск по строке, поисковик выделяет слова, которые одинаковы со словами из введенной строки. 
Эти выделения визуально выглядят как желтые подчеркивания, на практике же важно, что, в наших данных может присутствовать
нежелательный html-код. Функция del\_highlightings(line) удаляет из line все все такие выделения и возвращает строку без них.\\  
\newline
\fbox{\lstinputlisting[language=Python, title = wos\_parser.article\_parser(), firstline=38, lastline=56]{art_wos_parser.py}}
\newline
\newline
Наконец, функция wos\_parser.correct\_articles принимает массив articles, в котором могут быть пропуски на местах названий 
статей или имен авторов. Такие статьи не нужны ни для построения графа, ни для проведения классификации, поэтому они удаляются из
articles. Возвращает данная функция статьи, откорректированные таким образом.\\
\newline
\fbox{\lstinputlisting[language=Python, title = wos\_parser.correct\_articles(), firstline=58, lastline=72]{art_wos_parser.py}}
\newline
\hfill \break
Функция correct\_authors() приводит авторов к такому виду, чтобы статью можно было однозначно идентифицировать. Так, на сайте WoS
у одной и той же статьи может быть как 10 авторов, так и всего 2, а остальные скрыты за подписью "et al.". Также имена авторов 
могут быть написаны как в полной форме, так и в сокращении. Для того, чтобы считать такие статьи одинаковыми, эта функция выполняет
разбор случаев. А именно, все авторы приводятся к такому виду: фамилия начинается с заглавной букву, остальные строчные; если у 
автора присутствует имя, то берется только его первая буква, остальные отбрасываются; если есть второе имя, то с ним происходит 
то же самое. Все остальное, если оно есть, отбрасывается.\\
Благодаря данной процедуре, не учитываются одни и те же статьи дважды. Как именно отбрасывать ненужные статьи, описано
немного ниже.\\
\newline
\newline
\subsection{Граф цитирования}
В main.py для построения графа используется метод wos\_graph.build\_graph(). Он строит граф и сохраняет его в формате
gexf. С помощью программы Gephi можно легко визуализировать построенный граф. Метод возвращает массив статей без повторений.\\
В данной функции используются еще 2 абстрактных класса, описанные в файле abstract\_data.py\\
\newline 
\fbox{\lstinputlisting[language=Python, firstline=5, lastline=10]{art_abstract_data.py}}
\newline
Класс CitedReference - контейнер для хранения ссылки на статью. Он имеет поля name и author.\\
Класс AVertex используется для представления статьи как вершины графа, он имеет поля name и author и indeg (количество входящих ребер).\\ 
Несмотря на то, что по своей сути эти классы одинаковы, они не объединены в один. Мотивацей к этому послужило желание увеличить 
абстракцию данных и понятность кода.\\
Рассмотрим код начала функции build\_graph().\\
\newline
\fbox{\lstinputlisting[language=Python, firstline=1, lastline=12]{art_wos_graph.py}}
\newline
\hfill \break
Функция del\_equal\_articles(articles) принимает массив со статьями articles и возвращает массив без повторений (в смысле, описанном
выше). Две статьи считаются одинаковыми, если их названия совпадают, а множество авторов одной статьи является подмножеством 
множества авторов другой статьи. Авторы считаются одинаковыми, если их фамилии совпадают, а сокращения имен либо полностью совпадают,
либо одно из сокращений включает другое. Например , авторы "Abba G H" и "Abba G" считаются одинаковыми, а "Abba G H" и "Abba G M" нет.\\
v\_number - хеш-таблица, отвечающая за то, чтобы при добавлении новой статьи определять, добавлена ли она граф. Например, если 
\textbf{A}, \textbf{B}, \textbf{C} - статьи, "->" - ссылка одной статьи на другую, то при добавлении "\textbf{A} -> \textbf{B}" 
мы добавим в граф как вершину (статью) \textbf{A}, так и \textbf{B}. Тогда при добавлении "\textbf{C} -> \textbf{B}" вершина 
\textbf{B} не должна дублироваться.\\
Цикл for добавляет в массив с вершинами vertexes статьи, которые были скачаны по запросу пользователя.\\
После добавляются статьи, на которые они ссылаются\\
\newline
\fbox{\lstinputlisting[language=Python, firstline=13, lastline=36]{art_wos_graph.py}}
\newline
Данный код обрабатывает каждую из статей, проходя по всем статьям, на которые она ссылается. Если такая статья уже есть в графе,
то ее степень увеличивается и добавляется соответствующее ребро в массив с ребрами edges. Если же ее нет в графе, вершина добавляется 
в массив vertexes и в хеш-таблицу v\_number, также добавляется ребро и входящая степень инициализиуется единицей.\\
Таким образом, после выполнения данного кода строятся массивы с вершинами vertexes и с ребрами edges. Для построения графа 
используется библиотека NetworkX.\\
\newline
\fbox{\lstinputlisting[language=Python, firstline=38, lastline=60]{art_wos_graph.py}}
\newline
Имя вершины в графе записывается в виде <title> | <authors>. Размер вершины коррелирует с числом входящих ребер indeg.\\
После создания графа он сохраняется в формате gexf для дальнейшего использования.\\
\subsection{Кластеризация}
Наконец, рассмотрим часть программы, ответственную за кластеризацию.\\
Метод wos\_clasterization.build\_csv() используется для построения csv файла. С файлом в таком формате работать намного удобнее, чем с
массивом, в частности, появляется возможность использовать библиотеку pandas.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.build\_csv(), firstline=1, lastline=16]{art_wos_clasterization.py}}
\newline
Функция del\_empty\_abstracts() удаляет из массива статьи с пустым описанием. Для того, чтобы улучшить результат кластеризации,
в конец каждой аннотации добавляется название статьи.\\
\hfill \break
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (1), firstline=18, lastline=31]{art_wos_clasterization.py}}
\newline
\hfill \break
Создается датафрейм на основе подготовленного раннее csv-файла. Используя модуль nltk, подгружаются пунктуация и стоп-слова. Также 
инициализируется стеммер. Функция local\_stop\_words() проверяет, является ли слово искусственным стоп-словом. Иногда в кластеры попадают
слова, которые явно не имеют оригинальности. Для того, чтобы избавиться от таких слов, и существует данная функция.
\hfill \break
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (2), firstline=33, lastline=54]{art_wos_clasterization.py}}
\newline
\hfill \break
Функция tokenize\_and\_stem() разделяется слова на термы, удаляет из них как стоп-слова из модуля nltk, так и локальные стоп-слова.
Также из множества термов удаляются те, в которых есть апостроф или в которых нет латинских букв. В конце функции термы подвергаются 
стеммингу.\\
Функция tokenize() делает все то же самое, кроме стемминга.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (3), firstline=57, lastline=69]{art_wos_clasterization.py}}
\newline
\hfill \break
totalvocab\_stemmed - массив с термами из аннотаций, которые были переданы в tokenize\_and\_stem().\\
totalvocab\_tokenized - массив с термами из аннотаций, которые были переданы в tokenize().\\
vocab\_frame - датафрейм с одной колонкой: термами из массива totalvocab\_tokenized. Индексами в этом датафрейме служат слова, 
соответствующие словам в колонках, но подвергнутым стеммингу.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (4), firstline=71, lastline=82]{art_wos_clasterization.py}}
\newline
\hfill \break
TfidfVectorizer - метод из библиотеки sklearn. Он реализует построение модели $TF\text{-}IDF$. Параметры:\\
\textbf{max\_df} - максимальное значение $TF\text{-}IDF$\\
\textbf{min\_df} - минимальное значение $TF\text{-}IDF$\\
\textbf{stop\_words} - стоп-слова\\
\textbf{use\_idf} - использовать ли значение $IDF$\\
\textbf{tokenizer} - функция для разделения документов на термы\\
\textbf{ngram\_range} - создание N-грамм: сочетаний из термов\\
С помощью метода fit\_transform создается $TF\text{-}IDF$ матрица на основе аннотаций.\\
cosine\_similarity - расстояние между каждым из термов\\
dist - то, насколько термы схожи друг с другом\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (5), firstline=83, lastline=94]{art_wos_clasterization.py}}
\newline
\hfill \break
С помощью метода KMeans из sklearn производится кластеризацию статей. $k$ выбирается от 1 до 10 и для каждого $k$ в distortions
записываются суммы квадратов расстояний от элементов кластеров до их цетров. Затем применяется метод "локтя" и строим график. Так, для запроса 
Dijkstra's algorithm график схлж с рис. 7 (иногда KMeans может попадать в локальные минимумы, поэтому графики могут
быть различны).\\

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.5]{./images/em.png}
  \caption{Elbow method для 'Dijkstra's algorithm'}
\end{figure}

\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (6), firstline=96, lastline=103]{art_wos_clasterization.py}}
\newline
\hfill \break
Пользователь выбирает количество кластеров, которое ему кажется оптимальным. Затем обученная модель сохраняется (для возможного
переиспользования).\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (7), firstline=105, lastline=130]{art_wos_clasterization.py}}
\newline
\hfill \break
Выводятся термы, которые соответствуют каждому из кластеров. Если пользователь также передал параметр "showts", для каждого
кластера выводится список статей, которые в него входят.\\
Пример кластеризации будет в следующей главе.\\
Реализация алгоритма LDA с помощью библиотеки gensim.\\
Разделение аннотаций на термы, удаление стоп-слов.
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (8), firstline=132, lastline=136]{art_wos_clasterization.py}}
\newline
\hfill \break
В словарь gensim добавляются термы, они фультруются по количеству употреблений: нижнего ограничения нет, верхнее - употреблено не более
чем в половине абстрактов.\\
doc2bow - возвращает кортеж: (id слова, количество употреблений каждого слова в абстрактах)
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (9), firstline=137, lastline=139]{art_wos_clasterization.py}}
\newline
\hfill \break
Модель LDA на основе аннотаций. Параметры:\\
\textbf{corpus} - словарь с id слов и количеством их использований\\
\textbf{id2word} - словарь, ставящий с соответствие каждому id само слово\\
\textbf{num\_topics} - количество кластеров\\
\textbf{update\_every} - количество итераций между обновлениями кластеров\\
\textbf{chunksize} - размер буффера для каждого обновления\\
\textbf{passes} - количество прохождений алгоритма по corpus\\
После создания модели получается матрица с топ-словами для каждого из кластеров, производится их вывод на экран\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (10), firstline=141, lastline=148]{art_wos_clasterization.py}}
\newline
\hfill \break
Реализация MDS с помощью библиотеки sklearn. Параметры:\\
\textbf{n\_components} - размерность пространства, получаемого на выходе алгоритма\\
\textbf{dissimilarity = "precomputed"} - массив расстояний уже предподсчитан и подается в функцию mds.fit\_transform()\\
\textbf{random\_state} - генератор для инициализации кластеров\\ 
xs, ys - координаты вершин по осям X и Y\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (11), firstline=150, lastline=153]{art_wos_clasterization.py}}
\newline
\hfill \break
\hfill \break
\hfill \break
\hfill \break
Построение коллекций цветов и имен кластеров.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (12), firstline=155, lastline=161]{art_wos_clasterization.py}}
\newline
\hfill \break
Построение графика на основе этих данных. Вершины, лежащие в одном кластере, окрашены в один цвет.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (13), firstline=163, lastline=169]{art_wos_clasterization.py}}
\newline
\hfill \break
Вывод легенды, сохрание построенного графика.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (14), firstline=171, lastline=175]{art_wos_clasterization.py}}
\newline
\hfill \break
Построение модели HDC по матрице расстояний dist с помощью метода ward() из библиотеки scypy.\\
По полученной матрице строится дендрограмма.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (15), firstline=178, lastline=180]{art_wos_clasterization.py}}
\newline
\hfill \break
Сохраняем построенный график.\\
\newline
\fbox{\lstinputlisting[language=Python, title=wos\_clasterization.article\_clasterization() (16), firstline=182, lastline=185]{art_wos_clasterization.py}}
\newline
\hfill \break
\hfill \break
\hfill \break

\section{Анализ результатов}
В качестве примера работы программы рассмотрим ее вывод при вводе запроса 'Dijkstra's algorithm' и выбора трех кластеров.\\
Вывод результов работы алгоритмов \textbf{K-Means} и \textbf{LDA} показан на рисунках 8 и 9.\\
Первый из кластеров на рисунке 9 выводит наиболее общие данные. Можно предположить, что статьи, соответствующие этому кластеру, 
описывают сам алгоритм Дейкстры, как-то его анализируют или улучшают.\\
Второй из кластеров, скорее всего, описывает применение алгоритма Дейкстры в технологиях, так или иначе связанных с Wi-Fi сетями и
датчиками.\\
Третий кластер, по-видимому, связан с траффиком и протоколами.\\
Как видно, разные алгоритмы по-разному кластеризируют статьи. Для того, чтобы в дальнейшем улучшать результаты кластеризации, на
мой взгляд, лучше подходит \textbf{LDA} из-за его гибкости.\\

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.6]{./images/kmeansexample.png}
  \caption{K-Means для 'Dijkstra's algoritm'}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.6]{./images/ldaexample.png}
  \caption{LDA для 'Dijkstra's algorithm'}
\end{figure}

На рисунках 10 и 11 показан граф, построенный на основе статей, выданных по данному запросу. Граф построен с помощью программы Gephi.\\
При наведении на вершину показывается название статьи ее авторы. Размер вершины зависит от количества входящих ребер, ее цвет от того
выдана ли статья, соответствующая вершине, в поиске WoS по запросу пользователя, или же эта статья нашлась только в тех, на которые
ссылались. Белые вершины - статьи первого типа, зеленые - второго.\\
Как видно, есть статья, на которую очень много ссылаются. Автором данной статьи является сам Дейкстра.\\

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{./images/graph1.png}
    \caption{Граф для 'Dijkstra's algorithm'}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{./images/graph2.png}
    \caption{Статья Дейкстры}
  \end{minipage}
\end{figure}

\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\textbf{MDS} для данной выборки показан на рисунке 12.\\
Как видно, алгоритм показывает неплохой, но не самый лучший результат. Связано это с тем, что, во-первых, размерность исходного пространства - пространства
термов - огромна. Во-вторых, на самом деле множество статей не имеет явно выраженных кластеров, все статьи по данному запросу, в общем-то,
сильно схожи.\\
В любом случае, результат есть: один из кластеров (черный), в основном, занимает места, находящиеся около границы построенного эллипса,
второй (красный) места в правом нижнем углу, третий (фиолетовый) оставшуюся внутренность эллипса.\\


\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.4]{./images/mdsexample.png}
  \caption{MDS для 'Dijkstra's algorithm'}
\end{figure}

\section{Заключение}
В работе реализована программа, которая получает информацию об интересующих пользователя статьях с сайта  Web of Science. 
На основе этой иноформации программа строит граф цитирований статей и проводит их кластеризацию.\\
Описаны методы, которые обычно используются для данного класса задач: алгоритмы кластеризации (K-Means, latent Dirichlet
allocation) и анализа (multidimensional scaling, hierarchical document clustering).\\ 
Приведена инструкция по написанию программы.\\

\medskip
\renewcommand{\refname}{Список литературы}
{\footnotesize
\begin{thebibliography}{99}%Пример оформления списка литературы с помощью пакета amsbib. Для публикаций на русском языке в скобках приводится вариант на англ. языке, если есть (примеры 10 и 12), или транслитерация(примеры 3, 6, 7 и др.).

\RBibitem{tfidf}
\by Sp\"{a}rk~K.J
\paper A statistical interpretation of term specificity and its application in retrieval
\jour Journal of Documentation
\vol 60
\issue 5
\yr 2004
\pages 493--502

\RBibitem{klast}
\by Andreev A., Berezkin D., Morozov V., Simakov K.
\paper The method of clustering texts collections and clusters annotating 

\RBibitem{kmeans}
\by MacQUEEN J.
\paper Some methods for classification and analysis of multivariate observations

\RBibitem{lda}
\by Diane J. Hu
\paper Latent Dirichlet Allocation for Text, Images, and Music

\RBibitem{lda2}
\by Blei D.M., Ng A.Y., Jordan M.I.
\paper Latent Dirichlet Allocation
\jour Journal of Machine Learning Research
\issue 3
\yr 2003
\pages 993--1022

\RBibitem{mds}
\by Kruskal J.B.
\paper Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis
\jour Psychometrika
\vol 29
\issue 1
\yr 1964
\pages 1--27

\RBibitem{hdc}
\by Fung B.C.M., Wang K., Ester M.
\paper Hierarchical Document Clustering 

\RBibitem{brros}
\by Rose B.
\paper Document Clustering with Python 
\yr[Online] Available: http://brandonrose.org/clustering


\end{thebibliography}}
\vskip 8pt
   \smallskip\hrule width 6cm
\medskip

\end{document}



